{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from motifcluster import clustering as mccl\n",
    "from motifcluster import motifadjacency as mcmo\n",
    "from motifcluster import utils as mcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:Cora()\n",
      "====================\n",
      "Number of graphs:1\n",
      "Number of features:1433\n",
      "Number of classes:7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "=====================\n",
      "Number of nodes:2708\n",
      "Number of edges:10556\n",
      "Average node degree:3.90\n",
      "Number of training nodes:140\n",
      "Training node label rate:0.05\n",
      "Contains isolated nodes:False\n",
      "Contains self-loops:False\n",
      "Is undirected:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\code\\conda\\anaconda\\envs\\pyg\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
      "  warnings.warn(out)\n",
      "D:\\code\\conda\\anaconda\\envs\\pyg\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "# NormalizeFeatures()进行节点特征归一化，使特征总和为1\n",
    "dataset = Planetoid(root='dataset',name='Cora',transform=NormalizeFeatures())\n",
    "\n",
    "print(f'dataset:{dataset}')\n",
    "print('====================')\n",
    "print(f'Number of graphs:{len(dataset)}')\n",
    "print(f'Number of features:{dataset.num_features}')\n",
    "print(f'Number of classes:{dataset.num_classes}')\n",
    "\n",
    "# 得到第一个graph对象\n",
    "data = dataset[0]\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=====================')\n",
    "print(f'Number of nodes:{data.num_nodes}')\n",
    "print(f'Number of edges:{data.num_edges}')\n",
    "print(f'Average node degree:{data.num_edges/data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes:{data.train_mask.sum()}')\n",
    "print(f'Training node label rate:{int(data.train_mask.sum())/data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes:{data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops:{data.contains_self_loops()}')\n",
    "print(f'Is undirected:{data.is_undirected()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels):\n",
    "        super(GCN,self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_features,hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels,dataset.num_classes)\n",
    "\n",
    "    #     # 加入边权重的\n",
    "    def forward(self,x,edge_index,edge_weight):\n",
    "        x = self.conv1(x,edge_index,edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x,p=0.5,training=self.training)\n",
    "        x = self.conv2(x,edge_index,edge_weight)\n",
    "        return x\n",
    "\n",
    "# 未加入边权重的\n",
    "#     def forward(self,x,edge_index):\n",
    "#         x = self.conv1(x,edge_index)\n",
    "#         x = x.relu()\n",
    "#         x = F.dropout(x,p=0.5,training=self.training)\n",
    "#         x = self.conv2(x,edge_index)\n",
    "#         return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([ 1.,  1.,  1.,  ..., 34.,  4.,  5.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 使用MAM\n",
    "adj = data.edge_index\n",
    "print(data.edge_weight)\n",
    "adj = torch_geometric.utils.to_scipy_sparse_matrix(data.edge_index)\n",
    "mam = mcmo.build_motif_adjacency_matrix(adj,motif_name='M4',motif_type='func',mam_method='sparse',mam_weight_type='mean')\n",
    "mam1 = mcmo.build_motif_adjacency_matrix(adj,motif_name='M13',motif_type='struc',mam_method='sparse',mam_weight_type='mean')\n",
    "# mam_edge_index,mam_edge_weight = torch_geometric.utils.from_scipy_sparse_matrix(mam)\n",
    "# print(mam_edge_weight)\n",
    "# 加上原邻接矩阵与边权重\n",
    "inte_adj = adj + mam + mam1\n",
    "mam_edge_index,mam_edge_weight = torch_geometric.utils.from_scipy_sparse_matrix(inte_adj)\n",
    "print(mam_edge_weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:001,Loss:1.95\n",
      "Epoch:002,Loss:1.94\n",
      "Epoch:003,Loss:1.94\n",
      "Epoch:004,Loss:1.93\n",
      "Epoch:005,Loss:1.93\n",
      "Epoch:006,Loss:1.92\n",
      "Epoch:007,Loss:1.92\n",
      "Epoch:008,Loss:1.91\n",
      "Epoch:009,Loss:1.90\n",
      "Epoch:010,Loss:1.90\n",
      "Epoch:011,Loss:1.89\n",
      "Epoch:012,Loss:1.88\n",
      "Epoch:013,Loss:1.88\n",
      "Epoch:014,Loss:1.87\n",
      "Epoch:015,Loss:1.86\n",
      "Epoch:016,Loss:1.86\n",
      "Epoch:017,Loss:1.84\n",
      "Epoch:018,Loss:1.85\n",
      "Epoch:019,Loss:1.84\n",
      "Epoch:020,Loss:1.83\n",
      "Epoch:021,Loss:1.80\n",
      "Epoch:022,Loss:1.81\n",
      "Epoch:023,Loss:1.80\n",
      "Epoch:024,Loss:1.79\n",
      "Epoch:025,Loss:1.80\n",
      "Epoch:026,Loss:1.78\n",
      "Epoch:027,Loss:1.76\n",
      "Epoch:028,Loss:1.74\n",
      "Epoch:029,Loss:1.75\n",
      "Epoch:030,Loss:1.74\n",
      "Epoch:031,Loss:1.72\n",
      "Epoch:032,Loss:1.70\n",
      "Epoch:033,Loss:1.70\n",
      "Epoch:034,Loss:1.70\n",
      "Epoch:035,Loss:1.68\n",
      "Epoch:036,Loss:1.66\n",
      "Epoch:037,Loss:1.63\n",
      "Epoch:038,Loss:1.63\n",
      "Epoch:039,Loss:1.61\n",
      "Epoch:040,Loss:1.63\n",
      "Epoch:041,Loss:1.59\n",
      "Epoch:042,Loss:1.60\n",
      "Epoch:043,Loss:1.56\n",
      "Epoch:044,Loss:1.56\n",
      "Epoch:045,Loss:1.56\n",
      "Epoch:046,Loss:1.51\n",
      "Epoch:047,Loss:1.49\n",
      "Epoch:048,Loss:1.48\n",
      "Epoch:049,Loss:1.51\n",
      "Epoch:050,Loss:1.47\n",
      "Epoch:051,Loss:1.46\n",
      "Epoch:052,Loss:1.47\n",
      "Epoch:053,Loss:1.45\n",
      "Epoch:054,Loss:1.43\n",
      "Epoch:055,Loss:1.40\n",
      "Epoch:056,Loss:1.40\n",
      "Epoch:057,Loss:1.38\n",
      "Epoch:058,Loss:1.37\n",
      "Epoch:059,Loss:1.37\n",
      "Epoch:060,Loss:1.34\n",
      "Epoch:061,Loss:1.31\n",
      "Epoch:062,Loss:1.31\n",
      "Epoch:063,Loss:1.30\n",
      "Epoch:064,Loss:1.28\n",
      "Epoch:065,Loss:1.24\n",
      "Epoch:066,Loss:1.24\n",
      "Epoch:067,Loss:1.24\n",
      "Epoch:068,Loss:1.26\n",
      "Epoch:069,Loss:1.22\n",
      "Epoch:070,Loss:1.22\n",
      "Epoch:071,Loss:1.21\n",
      "Epoch:072,Loss:1.16\n",
      "Epoch:073,Loss:1.19\n",
      "Epoch:074,Loss:1.16\n",
      "Epoch:075,Loss:1.14\n",
      "Epoch:076,Loss:1.17\n",
      "Epoch:077,Loss:1.15\n",
      "Epoch:078,Loss:1.16\n",
      "Epoch:079,Loss:1.13\n",
      "Epoch:080,Loss:1.04\n",
      "Epoch:081,Loss:1.09\n",
      "Epoch:082,Loss:1.10\n",
      "Epoch:083,Loss:1.06\n",
      "Epoch:084,Loss:1.02\n",
      "Epoch:085,Loss:1.07\n",
      "Epoch:086,Loss:1.06\n",
      "Epoch:087,Loss:1.03\n",
      "Epoch:088,Loss:1.02\n",
      "Epoch:089,Loss:1.03\n",
      "Epoch:090,Loss:1.04\n",
      "Epoch:091,Loss:0.99\n",
      "Epoch:092,Loss:0.96\n",
      "Epoch:093,Loss:0.95\n",
      "Epoch:094,Loss:0.97\n",
      "Epoch:095,Loss:0.97\n",
      "Epoch:096,Loss:0.95\n",
      "Epoch:097,Loss:0.97\n",
      "Epoch:098,Loss:0.93\n",
      "Epoch:099,Loss:0.92\n",
      "Epoch:100,Loss:0.91\n",
      "Epoch:101,Loss:0.90\n",
      "Epoch:102,Loss:0.92\n",
      "Epoch:103,Loss:0.89\n",
      "Epoch:104,Loss:0.90\n",
      "Epoch:105,Loss:0.86\n",
      "Epoch:106,Loss:0.87\n",
      "Epoch:107,Loss:0.83\n",
      "Epoch:108,Loss:0.87\n",
      "Epoch:109,Loss:0.83\n",
      "Epoch:110,Loss:0.86\n",
      "Epoch:111,Loss:0.84\n",
      "Epoch:112,Loss:0.83\n",
      "Epoch:113,Loss:0.82\n",
      "Epoch:114,Loss:0.85\n",
      "Epoch:115,Loss:0.83\n",
      "Epoch:116,Loss:0.85\n",
      "Epoch:117,Loss:0.84\n",
      "Epoch:118,Loss:0.80\n",
      "Epoch:119,Loss:0.82\n",
      "Epoch:120,Loss:0.78\n",
      "Epoch:121,Loss:0.79\n",
      "Epoch:122,Loss:0.77\n",
      "Epoch:123,Loss:0.77\n",
      "Epoch:124,Loss:0.77\n",
      "Epoch:125,Loss:0.75\n",
      "Epoch:126,Loss:0.79\n",
      "Epoch:127,Loss:0.76\n",
      "Epoch:128,Loss:0.79\n",
      "Epoch:129,Loss:0.79\n",
      "Epoch:130,Loss:0.75\n",
      "Epoch:131,Loss:0.77\n",
      "Epoch:132,Loss:0.70\n",
      "Epoch:133,Loss:0.69\n",
      "Epoch:134,Loss:0.75\n",
      "Epoch:135,Loss:0.73\n",
      "Epoch:136,Loss:0.70\n",
      "Epoch:137,Loss:0.71\n",
      "Epoch:138,Loss:0.74\n",
      "Epoch:139,Loss:0.70\n",
      "Epoch:140,Loss:0.72\n",
      "Epoch:141,Loss:0.68\n",
      "Epoch:142,Loss:0.67\n",
      "Epoch:143,Loss:0.73\n",
      "Epoch:144,Loss:0.73\n",
      "Epoch:145,Loss:0.68\n",
      "Epoch:146,Loss:0.70\n",
      "Epoch:147,Loss:0.72\n",
      "Epoch:148,Loss:0.69\n",
      "Epoch:149,Loss:0.66\n",
      "Epoch:150,Loss:0.67\n",
      "Epoch:151,Loss:0.65\n",
      "Epoch:152,Loss:0.71\n",
      "Epoch:153,Loss:0.69\n",
      "Epoch:154,Loss:0.67\n",
      "Epoch:155,Loss:0.67\n",
      "Epoch:156,Loss:0.66\n",
      "Epoch:157,Loss:0.66\n",
      "Epoch:158,Loss:0.67\n",
      "Epoch:159,Loss:0.65\n",
      "Epoch:160,Loss:0.63\n",
      "Epoch:161,Loss:0.61\n",
      "Epoch:162,Loss:0.61\n",
      "Epoch:163,Loss:0.64\n",
      "Epoch:164,Loss:0.65\n",
      "Epoch:165,Loss:0.64\n",
      "Epoch:166,Loss:0.67\n",
      "Epoch:167,Loss:0.59\n",
      "Epoch:168,Loss:0.70\n",
      "Epoch:169,Loss:0.65\n",
      "Epoch:170,Loss:0.60\n",
      "Epoch:171,Loss:0.67\n",
      "Epoch:172,Loss:0.61\n",
      "Epoch:173,Loss:0.63\n",
      "Epoch:174,Loss:0.64\n",
      "Epoch:175,Loss:0.63\n",
      "Epoch:176,Loss:0.66\n",
      "Epoch:177,Loss:0.63\n",
      "Epoch:178,Loss:0.61\n",
      "Epoch:179,Loss:0.60\n",
      "Epoch:180,Loss:0.62\n",
      "Epoch:181,Loss:0.58\n",
      "Epoch:182,Loss:0.65\n",
      "Epoch:183,Loss:0.61\n",
      "Epoch:184,Loss:0.60\n",
      "Epoch:185,Loss:0.62\n",
      "Epoch:186,Loss:0.60\n",
      "Epoch:187,Loss:0.59\n",
      "Epoch:188,Loss:0.55\n",
      "Epoch:189,Loss:0.58\n",
      "Epoch:190,Loss:0.62\n",
      "Epoch:191,Loss:0.55\n",
      "Epoch:192,Loss:0.64\n",
      "Epoch:193,Loss:0.57\n",
      "Epoch:194,Loss:0.55\n",
      "Epoch:195,Loss:0.56\n",
      "Epoch:196,Loss:0.58\n",
      "Epoch:197,Loss:0.59\n",
      "Epoch:198,Loss:0.53\n",
      "Epoch:199,Loss:0.57\n",
      "Epoch:200,Loss:0.59\n"
     ]
    }
   ],
   "source": [
    "# GCN的训练\n",
    "model = GCN(hidden_channels=16)\n",
    "model = model.float()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01,weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# model, data = model.to(device), data.to(device)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # out = model(data.x,mam_edge_index)\n",
    "    out = model(data.x.float(),mam_edge_index,mam_edge_weight.float())\n",
    "    loss = criterion(out[data.train_mask],data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "for epoch in range(1,201):\n",
    "    loss = train()\n",
    "    print(f'Epoch:{epoch:03d},Loss:{loss:.2f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.776000\n"
     ]
    }
   ],
   "source": [
    "# GCN的测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    # out = model(data.x,data.edge_index)\n",
    "    out = model(data.x.float(),mam_edge_index,mam_edge_weight.float())\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "    test_acc = int(test_correct.sum())/int(data.test_mask.sum())\n",
    "    return test_acc\n",
    "\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy:{test_acc:.6f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
