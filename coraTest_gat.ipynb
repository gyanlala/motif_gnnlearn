{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from motifcluster import clustering as mccl\n",
    "from motifcluster import motifadjacency as mcmo\n",
    "from motifcluster import utils as mcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:Cora()\n",
      "====================\n",
      "Number of graphs:1\n",
      "Number of features:1433\n",
      "Number of classes:7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "=====================\n",
      "Number of nodes:2708\n",
      "Number of edges:10556\n",
      "Average node degree:3.90\n",
      "Number of training nodes:140\n",
      "Training node label rate:0.05\n",
      "Contains isolated nodes:False\n",
      "Contains self-loops:False\n",
      "Is undirected:True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "# NormalizeFeatures()进行节点特征归一化，使特征总和为1\n",
    "dataset = Planetoid(root='dataset',name='Cora',transform=NormalizeFeatures())\n",
    "\n",
    "print(f'dataset:{dataset}')\n",
    "print('====================')\n",
    "print(f'Number of graphs:{len(dataset)}')\n",
    "print(f'Number of features:{dataset.num_features}')\n",
    "print(f'Number of classes:{dataset.num_classes}')\n",
    "\n",
    "# 得到第一个graph对象\n",
    "data = dataset[0]\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=====================')\n",
    "print(f'Number of nodes:{data.num_nodes}')\n",
    "print(f'Number of edges:{data.num_edges}')\n",
    "print(f'Average node degree:{data.num_edges/data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes:{data.train_mask.sum()}')\n",
    "print(f'Training node label rate:{int(data.train_mask.sum())/data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes:{data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops:{data.contains_self_loops()}')\n",
    "print(f'Is undirected:{data.is_undirected()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 使用MAM\n",
    "adj = data.edge_index\n",
    "print(data.edge_weight)\n",
    "adj = torch_geometric.utils.to_scipy_sparse_matrix(data.edge_index)\n",
    "mam = mcmo.build_motif_adjacency_matrix(adj,motif_name='M4',motif_type='func',mam_method='sparse',mam_weight_type='product')\n",
    "# mam_edge_index,mam_edge_weight = torch_geometric.utils.from_scipy_sparse_matrix(mam)\n",
    "# print(mam_edge_weight)\n",
    "# 加上原邻接矩阵与边权重\n",
    "inte_adj = adj + mam\n",
    "mam_edge_index,mam_edge_weight = torch_geometric.utils.from_scipy_sparse_matrix(inte_adj)\n",
    "data.edge_index = mam_edge_index\n",
    "data.edge_weight = mam_edge_weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels):\n",
    "        super(GAT,self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(dataset.num_features,hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels,dataset.num_classes)\n",
    "\n",
    "    def forward(self,x,edge_index):\n",
    "        x = self.conv1(x,edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x,p=0.5,training=self.training)\n",
    "        x = self.conv2(x,edge_index)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:001,Loss:1.95\n",
      "Epoch:002,Loss:1.94\n",
      "Epoch:003,Loss:1.94\n",
      "Epoch:004,Loss:1.93\n",
      "Epoch:005,Loss:1.92\n",
      "Epoch:006,Loss:1.91\n",
      "Epoch:007,Loss:1.90\n",
      "Epoch:008,Loss:1.89\n",
      "Epoch:009,Loss:1.88\n",
      "Epoch:010,Loss:1.88\n",
      "Epoch:011,Loss:1.86\n",
      "Epoch:012,Loss:1.84\n",
      "Epoch:013,Loss:1.82\n",
      "Epoch:014,Loss:1.82\n",
      "Epoch:015,Loss:1.80\n",
      "Epoch:016,Loss:1.78\n",
      "Epoch:017,Loss:1.77\n",
      "Epoch:018,Loss:1.76\n",
      "Epoch:019,Loss:1.73\n",
      "Epoch:020,Loss:1.72\n",
      "Epoch:021,Loss:1.70\n",
      "Epoch:022,Loss:1.67\n",
      "Epoch:023,Loss:1.67\n",
      "Epoch:024,Loss:1.64\n",
      "Epoch:025,Loss:1.63\n",
      "Epoch:026,Loss:1.59\n",
      "Epoch:027,Loss:1.59\n",
      "Epoch:028,Loss:1.54\n",
      "Epoch:029,Loss:1.50\n",
      "Epoch:030,Loss:1.49\n",
      "Epoch:031,Loss:1.47\n",
      "Epoch:032,Loss:1.44\n",
      "Epoch:033,Loss:1.42\n",
      "Epoch:034,Loss:1.44\n",
      "Epoch:035,Loss:1.36\n",
      "Epoch:036,Loss:1.35\n",
      "Epoch:037,Loss:1.32\n",
      "Epoch:038,Loss:1.31\n",
      "Epoch:039,Loss:1.27\n",
      "Epoch:040,Loss:1.25\n",
      "Epoch:041,Loss:1.20\n",
      "Epoch:042,Loss:1.20\n",
      "Epoch:043,Loss:1.15\n",
      "Epoch:044,Loss:1.14\n",
      "Epoch:045,Loss:1.14\n",
      "Epoch:046,Loss:1.11\n",
      "Epoch:047,Loss:1.06\n",
      "Epoch:048,Loss:0.99\n",
      "Epoch:049,Loss:1.01\n",
      "Epoch:050,Loss:1.00\n",
      "Epoch:051,Loss:0.96\n",
      "Epoch:052,Loss:0.96\n",
      "Epoch:053,Loss:0.92\n",
      "Epoch:054,Loss:0.90\n",
      "Epoch:055,Loss:0.91\n",
      "Epoch:056,Loss:0.84\n",
      "Epoch:057,Loss:0.83\n",
      "Epoch:058,Loss:0.82\n",
      "Epoch:059,Loss:0.80\n",
      "Epoch:060,Loss:0.78\n",
      "Epoch:061,Loss:0.73\n",
      "Epoch:062,Loss:0.76\n",
      "Epoch:063,Loss:0.76\n",
      "Epoch:064,Loss:0.69\n",
      "Epoch:065,Loss:0.70\n",
      "Epoch:066,Loss:0.64\n",
      "Epoch:067,Loss:0.66\n",
      "Epoch:068,Loss:0.62\n",
      "Epoch:069,Loss:0.62\n",
      "Epoch:070,Loss:0.66\n",
      "Epoch:071,Loss:0.62\n",
      "Epoch:072,Loss:0.60\n",
      "Epoch:073,Loss:0.56\n",
      "Epoch:074,Loss:0.51\n",
      "Epoch:075,Loss:0.55\n",
      "Epoch:076,Loss:0.58\n",
      "Epoch:077,Loss:0.52\n",
      "Epoch:078,Loss:0.50\n",
      "Epoch:079,Loss:0.52\n",
      "Epoch:080,Loss:0.47\n",
      "Epoch:081,Loss:0.47\n",
      "Epoch:082,Loss:0.47\n",
      "Epoch:083,Loss:0.50\n",
      "Epoch:084,Loss:0.44\n",
      "Epoch:085,Loss:0.44\n",
      "Epoch:086,Loss:0.46\n",
      "Epoch:087,Loss:0.45\n",
      "Epoch:088,Loss:0.37\n",
      "Epoch:089,Loss:0.41\n",
      "Epoch:090,Loss:0.43\n",
      "Epoch:091,Loss:0.42\n",
      "Epoch:092,Loss:0.42\n",
      "Epoch:093,Loss:0.42\n",
      "Epoch:094,Loss:0.36\n",
      "Epoch:095,Loss:0.40\n",
      "Epoch:096,Loss:0.35\n",
      "Epoch:097,Loss:0.33\n",
      "Epoch:098,Loss:0.35\n",
      "Epoch:099,Loss:0.35\n",
      "Epoch:100,Loss:0.35\n",
      "Epoch:101,Loss:0.33\n",
      "Epoch:102,Loss:0.34\n",
      "Epoch:103,Loss:0.39\n",
      "Epoch:104,Loss:0.31\n",
      "Epoch:105,Loss:0.33\n",
      "Epoch:106,Loss:0.38\n",
      "Epoch:107,Loss:0.32\n",
      "Epoch:108,Loss:0.30\n",
      "Epoch:109,Loss:0.33\n",
      "Epoch:110,Loss:0.31\n",
      "Epoch:111,Loss:0.34\n",
      "Epoch:112,Loss:0.31\n",
      "Epoch:113,Loss:0.32\n",
      "Epoch:114,Loss:0.32\n",
      "Epoch:115,Loss:0.26\n",
      "Epoch:116,Loss:0.31\n",
      "Epoch:117,Loss:0.30\n",
      "Epoch:118,Loss:0.38\n",
      "Epoch:119,Loss:0.26\n",
      "Epoch:120,Loss:0.32\n",
      "Epoch:121,Loss:0.28\n",
      "Epoch:122,Loss:0.27\n",
      "Epoch:123,Loss:0.31\n",
      "Epoch:124,Loss:0.30\n",
      "Epoch:125,Loss:0.28\n",
      "Epoch:126,Loss:0.27\n",
      "Epoch:127,Loss:0.23\n",
      "Epoch:128,Loss:0.30\n",
      "Epoch:129,Loss:0.22\n",
      "Epoch:130,Loss:0.25\n",
      "Epoch:131,Loss:0.27\n",
      "Epoch:132,Loss:0.26\n",
      "Epoch:133,Loss:0.26\n",
      "Epoch:134,Loss:0.28\n",
      "Epoch:135,Loss:0.24\n",
      "Epoch:136,Loss:0.25\n",
      "Epoch:137,Loss:0.29\n",
      "Epoch:138,Loss:0.27\n",
      "Epoch:139,Loss:0.24\n",
      "Epoch:140,Loss:0.23\n",
      "Epoch:141,Loss:0.28\n",
      "Epoch:142,Loss:0.22\n",
      "Epoch:143,Loss:0.22\n",
      "Epoch:144,Loss:0.22\n",
      "Epoch:145,Loss:0.30\n",
      "Epoch:146,Loss:0.22\n",
      "Epoch:147,Loss:0.26\n",
      "Epoch:148,Loss:0.24\n",
      "Epoch:149,Loss:0.20\n",
      "Epoch:150,Loss:0.27\n",
      "Epoch:151,Loss:0.22\n",
      "Epoch:152,Loss:0.26\n",
      "Epoch:153,Loss:0.22\n",
      "Epoch:154,Loss:0.22\n",
      "Epoch:155,Loss:0.24\n",
      "Epoch:156,Loss:0.21\n",
      "Epoch:157,Loss:0.21\n",
      "Epoch:158,Loss:0.19\n",
      "Epoch:159,Loss:0.21\n",
      "Epoch:160,Loss:0.22\n",
      "Epoch:161,Loss:0.22\n",
      "Epoch:162,Loss:0.18\n",
      "Epoch:163,Loss:0.19\n",
      "Epoch:164,Loss:0.25\n",
      "Epoch:165,Loss:0.23\n",
      "Epoch:166,Loss:0.17\n",
      "Epoch:167,Loss:0.19\n",
      "Epoch:168,Loss:0.21\n",
      "Epoch:169,Loss:0.20\n",
      "Epoch:170,Loss:0.21\n",
      "Epoch:171,Loss:0.22\n",
      "Epoch:172,Loss:0.25\n",
      "Epoch:173,Loss:0.22\n",
      "Epoch:174,Loss:0.17\n",
      "Epoch:175,Loss:0.22\n",
      "Epoch:176,Loss:0.21\n",
      "Epoch:177,Loss:0.20\n",
      "Epoch:178,Loss:0.22\n",
      "Epoch:179,Loss:0.20\n",
      "Epoch:180,Loss:0.18\n",
      "Epoch:181,Loss:0.24\n",
      "Epoch:182,Loss:0.18\n",
      "Epoch:183,Loss:0.23\n",
      "Epoch:184,Loss:0.17\n",
      "Epoch:185,Loss:0.20\n",
      "Epoch:186,Loss:0.21\n",
      "Epoch:187,Loss:0.19\n",
      "Epoch:188,Loss:0.20\n",
      "Epoch:189,Loss:0.19\n",
      "Epoch:190,Loss:0.16\n",
      "Epoch:191,Loss:0.19\n",
      "Epoch:192,Loss:0.20\n",
      "Epoch:193,Loss:0.19\n",
      "Epoch:194,Loss:0.22\n",
      "Epoch:195,Loss:0.20\n",
      "Epoch:196,Loss:0.16\n",
      "Epoch:197,Loss:0.19\n",
      "Epoch:198,Loss:0.17\n",
      "Epoch:199,Loss:0.17\n",
      "Epoch:200,Loss:0.19\n",
      "Epoch:201,Loss:0.18\n",
      "Epoch:202,Loss:0.15\n",
      "Epoch:203,Loss:0.18\n",
      "Epoch:204,Loss:0.19\n",
      "Epoch:205,Loss:0.18\n",
      "Epoch:206,Loss:0.16\n",
      "Epoch:207,Loss:0.17\n",
      "Epoch:208,Loss:0.18\n",
      "Epoch:209,Loss:0.17\n",
      "Epoch:210,Loss:0.19\n",
      "Epoch:211,Loss:0.16\n",
      "Epoch:212,Loss:0.16\n",
      "Epoch:213,Loss:0.17\n",
      "Epoch:214,Loss:0.18\n",
      "Epoch:215,Loss:0.17\n",
      "Epoch:216,Loss:0.16\n",
      "Epoch:217,Loss:0.20\n",
      "Epoch:218,Loss:0.14\n",
      "Epoch:219,Loss:0.18\n",
      "Epoch:220,Loss:0.16\n",
      "Epoch:221,Loss:0.17\n",
      "Epoch:222,Loss:0.24\n",
      "Epoch:223,Loss:0.16\n",
      "Epoch:224,Loss:0.18\n",
      "Epoch:225,Loss:0.15\n",
      "Epoch:226,Loss:0.16\n",
      "Epoch:227,Loss:0.15\n",
      "Epoch:228,Loss:0.16\n",
      "Epoch:229,Loss:0.16\n",
      "Epoch:230,Loss:0.20\n",
      "Epoch:231,Loss:0.21\n",
      "Epoch:232,Loss:0.14\n",
      "Epoch:233,Loss:0.12\n",
      "Epoch:234,Loss:0.15\n",
      "Epoch:235,Loss:0.20\n",
      "Epoch:236,Loss:0.15\n",
      "Epoch:237,Loss:0.21\n",
      "Epoch:238,Loss:0.19\n",
      "Epoch:239,Loss:0.15\n",
      "Epoch:240,Loss:0.16\n",
      "Epoch:241,Loss:0.15\n",
      "Epoch:242,Loss:0.17\n",
      "Epoch:243,Loss:0.19\n",
      "Epoch:244,Loss:0.13\n",
      "Epoch:245,Loss:0.17\n",
      "Epoch:246,Loss:0.12\n",
      "Epoch:247,Loss:0.14\n",
      "Epoch:248,Loss:0.15\n",
      "Epoch:249,Loss:0.14\n",
      "Epoch:250,Loss:0.14\n",
      "Epoch:251,Loss:0.14\n",
      "Epoch:252,Loss:0.17\n",
      "Epoch:253,Loss:0.12\n",
      "Epoch:254,Loss:0.14\n",
      "Epoch:255,Loss:0.17\n",
      "Epoch:256,Loss:0.17\n",
      "Epoch:257,Loss:0.17\n",
      "Epoch:258,Loss:0.14\n",
      "Epoch:259,Loss:0.20\n",
      "Epoch:260,Loss:0.12\n",
      "Epoch:261,Loss:0.13\n",
      "Epoch:262,Loss:0.16\n",
      "Epoch:263,Loss:0.21\n",
      "Epoch:264,Loss:0.14\n",
      "Epoch:265,Loss:0.13\n",
      "Epoch:266,Loss:0.13\n",
      "Epoch:267,Loss:0.11\n",
      "Epoch:268,Loss:0.17\n",
      "Epoch:269,Loss:0.16\n",
      "Epoch:270,Loss:0.16\n",
      "Epoch:271,Loss:0.18\n",
      "Epoch:272,Loss:0.14\n",
      "Epoch:273,Loss:0.12\n",
      "Epoch:274,Loss:0.15\n",
      "Epoch:275,Loss:0.14\n",
      "Epoch:276,Loss:0.10\n",
      "Epoch:277,Loss:0.13\n",
      "Epoch:278,Loss:0.12\n",
      "Epoch:279,Loss:0.17\n",
      "Epoch:280,Loss:0.12\n",
      "Epoch:281,Loss:0.15\n",
      "Epoch:282,Loss:0.14\n",
      "Epoch:283,Loss:0.14\n",
      "Epoch:284,Loss:0.12\n",
      "Epoch:285,Loss:0.14\n",
      "Epoch:286,Loss:0.13\n",
      "Epoch:287,Loss:0.15\n",
      "Epoch:288,Loss:0.13\n",
      "Epoch:289,Loss:0.13\n",
      "Epoch:290,Loss:0.14\n",
      "Epoch:291,Loss:0.13\n",
      "Epoch:292,Loss:0.17\n",
      "Epoch:293,Loss:0.11\n",
      "Epoch:294,Loss:0.12\n",
      "Epoch:295,Loss:0.14\n",
      "Epoch:296,Loss:0.16\n",
      "Epoch:297,Loss:0.11\n",
      "Epoch:298,Loss:0.17\n",
      "Epoch:299,Loss:0.12\n",
      "Epoch:300,Loss:0.13\n"
     ]
    }
   ],
   "source": [
    "# GAT的训练\n",
    "model = GAT(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01,weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x,data.edge_index)\n",
    "    loss = criterion(out[data.train_mask],data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "for epoch in range(1,301):\n",
    "    loss = train()\n",
    "    print(f'Epoch:{epoch:03d},Loss:{loss:.2f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.7290\n"
     ]
    }
   ],
   "source": [
    "# GAT的测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x,data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "    test_acc = int(test_correct.sum())/int(data.test_mask.sum())\n",
    "    return test_acc\n",
    "\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy:{test_acc:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
